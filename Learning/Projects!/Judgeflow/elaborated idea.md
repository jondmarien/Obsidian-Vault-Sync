## Summary

The core opportunity lies in automating the most painful and time‑consuming parts of hackathon judging: manual eligibility checks and cheat‑detection, followed by rudimentary pre‑screening and note generation for judges. By leveraging Perplexity’s AI‑search APIs to scrape and semantically analyze Devpost data, code repos, social media profiles, and video transcripts, your tool can (1) flag potential ineligibility/cheating, (2) rank and shortlist entries, and (3) generate concise judge briefs. A razor‑focused MVP on “Automated Eligibility & Integrity Screening” unlocks immediate value, while a B2B SaaS model—partnering directly with platforms like Devpost or MLH—offers the fastest path to market.

---

## 1. Main Feature: Automated Eligibility & Integrity Screening

### 1.1 Judging Pain Points  
Hackathon organizers cite judging as their single biggest friction, often scrambling to recruit judges, plan time slots, and manually vet submissions on the day of the event citeturn0search0. Digital hackathons amplify cheating risks, forcing organizers to conduct manual “who‑did‑what” interviews for every winning project citeturn0search1.

### 1.2 Cheating & Ineligibility Flags  
- **Cheat detection**: Existing tools like Cheater Beater partially automate fraud flags by cross‑referencing Devpost submissions with GitHub activity citeturn0search21.  
- **Eligibility checks**: Devpost’s native dashboard offers only high‑level filters (e.g., geography, age) citeturn0search4, but deeper checks (e.g., participant employment, previous wins) require manual verification.

### 1.3 How Perplexity AI Helps  
Perplexity’s APIs excel at web‑scale semantic search and entity extraction, enabling you to:  
1. **Scrape & parse Devpost entries** via unofficial APIs citeturn0search2turn0search7.  
2. **Scan social profiles** (LinkedIn/Twitter) for disallowed affiliations or prior prizes.  
3. **Analyze README/video transcripts** for reused templates or off‑the‑shelf solutions.  
4. **Generate integrity flags** with confidence scores, triaging submissions for human review.

---

## 2. Secondary Feature: Project Shortlisting & Judge Briefs

### 2.1 Automated Scoring & Ranking  
By embedding simple rubric‑based prompts, the tool can assign early scores on impact, feasibility, and innovation—freeing judges from sifting through dozens of unvetted entries citeturn0search5.

### 2.2 Brief Generation  
Using AI summarization, produce 3–5 bullet “scorecards” per project, pulling key points from READMEs and demo videos. Similar LLM‑judge experiments have shown promise at specialized AI hackathons citeturn0search28.

---

## 3. Go‑to‑Market & Success Strategy

### 3.1 Partnership with Hackathon Platforms  
Pitch a white‑label integration to Devpost or MLH—embedding your screening widget directly into their manager dashboards. Devpost for Teams already streamlines judging logistics; your AI layer adds integrity checks and pre‑screening, plugging into existing workflows citeturn0search10.

### 3.2 B2B SaaS Model  
- **Per‑event licensing**: Fixed fee per hackathon, tiered by submission volume.  
- **Enterprise plans**: Annual subscriptions for large organizations running multiple events.

### 3.3 Roadmap & Metrics  
1. **MVP**: Core eligibility/cheating scanner + basic UI for human review.  
2. **Beta**: Add shortlisting/ranking and judge‑brief features.  
3. **Scale**: Expand to coding competitions (e.g., Kaggle, ICPC) and conferences.  
4. **KPIs**: Reduction in manual vetting time, judge satisfaction scores, partnership adoption rate.

---

## 4. Why This Will Succeed

1. **High‑impact pain point**: Judging delays frustrate organizers and sponsors alike citeturn0search0.  
2. **Differentiated capability**: Few solutions go beyond manual cheat‑checks; an AI‑powered, end‑to‑end screening + pre‑score pipeline is truly novel citeturn0search21.  
3. **Strategic partners**: Leveraging your teammate’s Perplexity campus connection smooths API access and early adopter buy‑in.  
4. **Scalable market**: Thousands of hackathons and coding contests run annually, all hungry for faster, fairer judging.

By zeroing in on “Automated Eligibility & Integrity Screening,” you maximize early impact and lay the technical foundation for expanded AI‑driven judging workflows.